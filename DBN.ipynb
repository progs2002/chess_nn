{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(773,100)\n",
    "        self.fc2 = nn.Linear(100,100)\n",
    "        self.fc3 = nn.Linear(100,100)\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbn = DBN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "from extractor import get_dataset\n",
    "path = 'dataset/games.pgn'\n",
    "num_games = 5000\n",
    "\n",
    "X, Y = get_dataset(path, num_games)\n",
    "X = X.type(torch.FloatTensor)\n",
    "Y = Y.type(torch.FloatTensor)\n",
    "len_data = X.shape[0]\n",
    "split = int(len_data*0.8)\n",
    "X_train = X[:split,:]\n",
    "X_test = X[split:,:]\n",
    "Y_train = Y[:split]\n",
    "Y_test = Y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "dbn_dataset = TensorDataset(X, torch.zeros((X.shape[0])))\n",
    "dbn_dataloader = DataLoader(dbn_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "train_datalaoder = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test, Y_test)\n",
    "test_datalaoder = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "[1,   500] loss: 147.163\n",
      "[1,  1000] loss: 128.240\n",
      "[1,  1500] loss: 126.818\n",
      "[1,  2000] loss: 125.984\n",
      "[1,  2500] loss: 125.103\n",
      "[1,  3000] loss: 125.597\n",
      "[1,  3500] loss: 124.717\n",
      "[1,  4000] loss: 123.507\n",
      "[1,  4500] loss: 122.823\n",
      "[1,  5000] loss: 122.342\n",
      "[1,  5500] loss: 121.864\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "[2,   500] loss: 120.832\n",
      "[2,  1000] loss: 121.319\n",
      "[2,  1500] loss: 119.341\n",
      "[2,  2000] loss: 119.132\n",
      "[2,  2500] loss: 117.348\n",
      "[2,  3000] loss: 114.828\n",
      "[2,  3500] loss: 112.589\n",
      "[2,  4000] loss: 111.336\n",
      "[2,  4500] loss: 110.533\n",
      "[2,  5000] loss: 109.831\n",
      "[2,  5500] loss: 109.059\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "[3,   500] loss: 106.252\n",
      "[3,  1000] loss: 105.160\n",
      "[3,  1500] loss: 104.471\n",
      "[3,  2000] loss: 103.167\n",
      "[3,  2500] loss: 102.346\n",
      "[3,  3000] loss: 101.144\n",
      "[3,  3500] loss: 100.525\n",
      "[3,  4000] loss: 100.206\n",
      "[3,  4500] loss: 98.488\n",
      "[3,  5000] loss: 98.456\n",
      "[3,  5500] loss: 97.619\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "[4,   500] loss: 96.220\n",
      "[4,  1000] loss: 95.548\n",
      "[4,  1500] loss: 94.688\n",
      "[4,  2000] loss: 93.896\n",
      "[4,  2500] loss: 93.931\n",
      "[4,  3000] loss: 93.138\n",
      "[4,  3500] loss: 92.960\n",
      "[4,  4000] loss: 92.219\n",
      "[4,  4500] loss: 91.959\n",
      "[4,  5000] loss: 91.362\n",
      "[4,  5500] loss: 91.056\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "[5,   500] loss: 89.906\n",
      "[5,  1000] loss: 89.605\n",
      "[5,  1500] loss: 89.381\n",
      "[5,  2000] loss: 88.970\n",
      "[5,  2500] loss: 88.457\n",
      "[5,  3000] loss: 88.253\n",
      "[5,  3500] loss: 87.902\n",
      "[5,  4000] loss: 88.000\n",
      "[5,  4500] loss: 87.807\n",
      "[5,  5000] loss: 87.703\n",
      "[5,  5500] loss: 87.421\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "[6,   500] loss: 86.936\n",
      "[6,  1000] loss: 86.523\n",
      "[6,  1500] loss: 86.329\n",
      "[6,  2000] loss: 86.290\n",
      "[6,  2500] loss: 85.934\n",
      "[6,  3000] loss: 85.982\n",
      "[6,  3500] loss: 85.718\n",
      "[6,  4000] loss: 85.541\n",
      "[6,  4500] loss: 84.902\n",
      "[6,  5000] loss: 85.217\n",
      "[6,  5500] loss: 84.870\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "[7,   500] loss: 84.793\n",
      "[7,  1000] loss: 84.251\n",
      "[7,  1500] loss: 84.280\n",
      "[7,  2000] loss: 83.838\n",
      "[7,  2500] loss: 83.816\n",
      "[7,  3000] loss: 83.634\n",
      "[7,  3500] loss: 83.443\n",
      "[7,  4000] loss: 83.215\n",
      "[7,  4500] loss: 83.465\n",
      "[7,  5000] loss: 83.149\n",
      "[7,  5500] loss: 82.651\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "[8,   500] loss: 82.717\n",
      "[8,  1000] loss: 82.840\n",
      "[8,  1500] loss: 82.410\n",
      "[8,  2000] loss: 82.351\n",
      "[8,  2500] loss: 82.154\n",
      "[8,  3000] loss: 82.050\n",
      "[8,  3500] loss: 82.417\n",
      "[8,  4000] loss: 81.770\n",
      "[8,  4500] loss: 81.673\n",
      "[8,  5000] loss: 81.536\n",
      "[8,  5500] loss: 81.324\n",
      "finished training layer 1\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "[1,   500] loss: 18.088\n",
      "[1,  1000] loss: 0.031\n",
      "[1,  1500] loss: 0.027\n",
      "[1,  2000] loss: 0.020\n",
      "[1,  2500] loss: 0.015\n",
      "[1,  3000] loss: 0.013\n",
      "[1,  3500] loss: 0.009\n",
      "[1,  4000] loss: 0.011\n",
      "[1,  4500] loss: 0.007\n",
      "[1,  5000] loss: 0.010\n",
      "[1,  5500] loss: 0.005\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "[2,   500] loss: 0.004\n",
      "[2,  1000] loss: 0.005\n",
      "[2,  1500] loss: 0.004\n",
      "[2,  2000] loss: 0.003\n",
      "[2,  2500] loss: 0.005\n",
      "[2,  3000] loss: 0.006\n",
      "[2,  3500] loss: 0.004\n",
      "[2,  4000] loss: 0.003\n",
      "[2,  4500] loss: 0.002\n",
      "[2,  5000] loss: 0.005\n",
      "[2,  5500] loss: 0.004\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "[3,   500] loss: 0.003\n",
      "[3,  1000] loss: 0.004\n",
      "[3,  1500] loss: 0.003\n",
      "[3,  2000] loss: 0.002\n",
      "[3,  2500] loss: 0.002\n",
      "[3,  3000] loss: 0.002\n",
      "[3,  3500] loss: 0.002\n",
      "[3,  4000] loss: 0.001\n",
      "[3,  4500] loss: 0.002\n",
      "[3,  5000] loss: 0.002\n",
      "[3,  5500] loss: 0.002\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "[4,   500] loss: 0.002\n",
      "[4,  1000] loss: 0.001\n",
      "[4,  1500] loss: 0.001\n",
      "[4,  2000] loss: 0.001\n",
      "[4,  2500] loss: 0.001\n",
      "[4,  3000] loss: 0.002\n",
      "[4,  3500] loss: 0.002\n",
      "[4,  4000] loss: 0.001\n",
      "[4,  4500] loss: 0.001\n",
      "[4,  5000] loss: 0.001\n",
      "[4,  5500] loss: 0.002\n",
      "finished training layer 2\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "[1,   500] loss: 1213.981\n",
      "[1,  1000] loss: 0.003\n",
      "[1,  1500] loss: 0.000\n",
      "[1,  2000] loss: 0.000\n",
      "[1,  2500] loss: 0.000\n",
      "[1,  3000] loss: 0.000\n",
      "[1,  3500] loss: 0.000\n",
      "[1,  4000] loss: 0.000\n",
      "[1,  4500] loss: 0.000\n",
      "[1,  5000] loss: 0.000\n",
      "[1,  5500] loss: 0.000\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "[2,   500] loss: 0.000\n",
      "[2,  1000] loss: 0.000\n",
      "[2,  1500] loss: 0.000\n",
      "[2,  2000] loss: 0.000\n",
      "[2,  2500] loss: 0.000\n",
      "[2,  3000] loss: 0.000\n",
      "[2,  3500] loss: 0.000\n",
      "[2,  4000] loss: 0.000\n",
      "[2,  4500] loss: 0.000\n",
      "[2,  5000] loss: 0.000\n",
      "[2,  5500] loss: 0.000\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "[3,   500] loss: 0.000\n",
      "[3,  1000] loss: 0.000\n",
      "[3,  1500] loss: 0.000\n",
      "[3,  2000] loss: 0.000\n",
      "[3,  2500] loss: 0.000\n",
      "[3,  3000] loss: 0.000\n",
      "[3,  3500] loss: 0.000\n",
      "[3,  4000] loss: 0.000\n",
      "[3,  4500] loss: 0.000\n",
      "[3,  5000] loss: 0.000\n",
      "[3,  5500] loss: 0.000\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "[4,   500] loss: 0.000\n",
      "[4,  1000] loss: 0.000\n",
      "[4,  1500] loss: 0.000\n",
      "[4,  2000] loss: 0.000\n",
      "[4,  2500] loss: 0.000\n",
      "[4,  3000] loss: 0.000\n",
      "[4,  3500] loss: 0.000\n",
      "[4,  4000] loss: 0.000\n",
      "[4,  4500] loss: 0.000\n",
      "[4,  5000] loss: 0.000\n",
      "[4,  5500] loss: 0.000\n",
      "finished training layer 3\n"
     ]
    }
   ],
   "source": [
    "#greedy training \n",
    "\n",
    "layers = [773,100,100,100]\n",
    "W, B, C = [], [], []\n",
    "\n",
    "for i in range(len(layers)-1):\n",
    "    w = torch.randn((layers[i],layers[i+1]),dtype=torch.float)\n",
    "    c = torch.randn((layers[i]),dtype=torch.float) if i==0 else B[i-1]\n",
    "    b = torch.randn((layers[i+1]),dtype=torch.float)\n",
    "    \n",
    "    parameters = [w, c, b]\n",
    "    for p in parameters:\n",
    "        p.requires_grad = True\n",
    "    lr = 0.1\n",
    "    optimizer = optim.SGD(parameters,lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    epochs = 8 if i == 0 else 4 \n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "        for batch, (x, _) in enumerate(dbn_dataloader):\n",
    "            if i > 0:\n",
    "                for j in range(len(W)):\n",
    "                    x = torch.relu(x @ W[j] + B[j])\n",
    "\n",
    "            probs = torch.relu((torch.relu(x @ w + b) @ w.T) + c)\n",
    "            loss = criterion(probs,x)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if batch % (num_games//10) == (num_games//10) - 1:\n",
    "                print(f'[{epoch + 1}, {batch + 1:5d}] loss: {running_loss / (num_games//10):.3f}')\n",
    "                running_loss = 0.0\n",
    "        losses.append(running_loss)\n",
    "        \n",
    "    print(f'finished training layer {i+1}')\n",
    "    W.append(w.clone().detach())\n",
    "    B.append(b.clone().detach())\n",
    "    C.append(c.clone().detach())\n",
    "    \n",
    "    for p in parameters: del p\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(105.9501)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(W[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
